或多或少从该领域诞生起我就一直参与其中，因此我不可避免地要介绍一些简要的历史背景。过程式方法已经存在了几十年，我们不应忽视其历史，因为其中有不少经验教训可供我们借鉴。当然，历史 “真理” 在如今并不总是成立，有时一项颠覆性的技术发明会将 “普遍认知” 变成纯粹的谬误。这是一把双刃剑：过程式方法在以前不适合它们的应用场景中发挥作用，但在它们曾经大放异彩的应用场景中，也可能失去优势。这些变化都是技术发展自然且不可避免的结果，但这使得了解这些方法以前在哪些地方得到应用、哪些地方未被考虑，以及最重要的原因，变得更加有用。 

在本章末尾，我将介绍当前的技术水平，并指出过程式方法在计算机图形学当前应用中的优缺点。了解历史往往对理解现状大有帮助，而审视当前形势则为本章画上一个合适的句号。


**<p align="center">“Let us consult The Writ Of Common Wisdom!”</p>** *<p align="center">Theodoric of York, Medieval Judge, played by Steve Martin on “Saturday Night Live” in 1978.</p>*


## 最早的时候

内容的程序化生成实际上是计算机图形学中许多早期实验的核心，因为在当时，内存极其短缺，纹理图像制作繁琐、存储不便且访问速度缓慢。计算机图形学在 20 世纪 70 年代才刚刚起步，那时计算机内存是以千字节而非千兆字节来衡量的，仅仅是存储输出图像就成了一个问题。在这种情况下，纹理图像根本无法负担。

柏林噪声——目前该领域最著名的算法——将在“噪声”一章中详细介绍，但其历史也值得在此一提。它源自一种需求，即为原本平淡无奇、单色且看起来像塑料质感的画面添加一些视觉细节。 

这些曲面是20世纪70年代和80年代计算机图形学的标志性特征。它是在1982年电影《Tron》的制作过程中发明的，这部电影中具有开创性的计算机图形序列由先锋公司MAGI制作，而柏林噪声作为图像合成器系统及其由Ken Perlin在纽约大学攻读博士学位期间开发的KPL像素处理语言的一个组件，被正式介绍给学术界。他的开创性文章“An Image Synthesizer” [ACM Proceedings of Siggraph 1985,
https://doi.org/10.1145/325334.325247]  中的图像轰动了整个图形学界，展示了一系列由相当简单的算法生成的、看起来很自然且视觉上有趣的图案，其种类之多令人惊讶。 

![2504281930 ](https://blogimgbeg.oss-cn-shanghai.aliyuncs.com/2025/04/28/1mg/2504281930-.png)
*Images from Perlin’s original paper, © ACM 1985. Used with permission.*


大约在同一时期，过程纹理被卢卡斯影业的计算机图形小组采用并用于商业用途，该小组后来成为皮克斯。1989年，皮克斯的渲染软件Renderman公开发布，使得过程纹理和建模技术可供迅速壮大的计算机图形专业人士群体使用。虽然许多其他创作者使用Renderman的过程方法创作出令人印象深刻的图像，但皮克斯自身仍然是其软件最具创造力和技术娴熟的用户之一。他们使用该软件来制作了几部开创性的作品，包括 1996 年的第一部完全计算机动画的长篇电影《玩具总动员》。
Renderman仍然是一款商业产品。其内部渲染算法如今与20世纪80年代时已完全不同，但它保留了一个可供终端用户轻松使用的程序框架，并且其现代的开放着色语言（OSL）与最初的Renderman着色语言（RSL）非常相似。（在过去，RSL简称为SL，因为它是当时唯一的着色语言。）

在这种背景下，值得一提的是，RenderMan框架最初是一个硬件项目。从1983年到1990年，皮克斯开发并制造了多个版本的定制硬件渲染器——皮克斯图像计算机。由于其价格极高，它从未在商业上取得成功，在项目终止前只售出了几百台。1987年发布的获奖短片《Red Dream》有一部分是使用其中一个版本的图像计算机渲染的，但大部分内容是使用通用计算机以软件形式渲染的，运行的正是后来的RenderMan。 


![2504282138 ](https://blogimgbeg.oss-cn-shanghai.aliyuncs.com/2025/04/28/1mg/2504282138-.png)

*A decommissioned Pixar Image Computer at a museum. (Public domain image)*


另一个值得一提的趣事是，1982年MAGI为电影《Tron》制作的视觉特效是使用光线追踪渲染的，这是一种成本相对较高的方法，直到大约20年后才在电影制作中得到常规使用。著名赛车场景中 “光轮摩托” 的设计，暴露出当时艺术家们可用的基本图形十分有限。当时没有多边形模型，只有简单立体形状的组合：长方体、球体、圆柱体、圆锥体和圆环体，以及它们之间的集合运算：并集、交集和差集。 

![2504282235 ](https://blogimgbeg.oss-cn-shanghai.aliyuncs.com/2025/04/28/1mg/2504282235-.png){align="center"}
*A faithful retro-style remake of a light cycle from the movie “Tron”, using the same modeling technique (CSG) and the same type of basic raytrace renderer (PovRay). (Getting permission from Disney to show an original shot wasn’t worth the hassle.)*


## 黑暗年代

*请注意，本小节标题“黑暗年代”仅指程序方法这一狭义领域。总体而言，计算机图形学在世纪之交取得了令人惊叹的进步，而追求照片级真实感的3D电脑游戏是价格合理的低成本图形处理单元（GPU）得以发展的最主要原因，正是这些GPU将我们带到了如今的水平。*

在20世纪90年代和21世纪初，内存和辅助存储设备的可用性不断提高，同时数字图像采集和数字图像编辑技术迅速发展，使得使用纹理图像来表现表面细节成为一件简单的事情，事实证明，这是一种快速且简便的方法，能够创建出具有视觉趣味性和复杂性的渲染效果。当面向个人计算机的低成本三维图形硬件开发出来后，它非常注重对纹理图像的高效处理。

基于图像的纹理映射方法迅速成为所有硬件辅助渲染的首选工具。过程式方法仅用于软件渲染，而当纹理图像变得易于处理时，即使在软件渲染中，它们也不再被使用。围绕Renderman的创意社区在使用和支持过程式纹理方面仍然很活跃，并且由于过程式方法的灵活性和较小的内存占用，在一些特定应用中仍受青睐。但从20世纪90年代中期开始，大多数计算机图形从业者完全转向使用纹理图像，计算机图形硬件的开发者也设计出能高效处理大量高分辨率图像数据的芯片。有一段时间，大多数图形加速器的芯片面积曾专门用于纹理图像处理。如今情况已有所不同，但大量的处理能力仍用于纹理处理，而且现代个人电脑显卡上的本地内存容量与主机系统中的内存容量相当，这在很大程度上是因为存储未压缩的纹理图像需要大量内存。在当今的图形处理器（GPU）上，传统的纹理密集型渲染方法之所以能实现高性能，很大程度上得益于具有宽且快速总线的本地内存，以及巧妙的内存管理和缓存策略。

基于图像的纹理仍然很重要，而且它们确实非常有用，但近十年来，另一种方法——程序化方法，完全得不到硬件的支持。我们目前的状况是，电脑游戏和视觉特效的计算机图形中，大多数“表面处理”严重倾向于使用大量高分辨率纹理，这些纹理通常由一小群专门使用二维图像编辑软件的纹理艺术家制作。程序化方法常常被直接否定，甚至都不被考虑。然而，基于图像的方法存在缺点，替代方法确实存在，而且它们正理所当然地重新引起人们的兴趣。

## 文艺复兴

随着开发的持续进行，基于图像的纹理映射方法越来越多地使用多重纹理映射技术，即通过组合多个纹理图像来创建表面的最终外观。这就要求在纹理组合（混合）方式上具备相当大的灵活性，而传统的固定功能渲染模式变得难以应对，因为需要设置大量的状态，并且要启用或禁用许多阶段。因此，GPU核心被设计为可编程单元，以适应所有这些需求，因为这是最灵活的解决方案。起初，这种可编程性对应用程序程序员是隐藏的，但过了一段时间后，它作为一个选项向程序员开放。标准化的“OpenGL ARB汇编语言”（“ARB”是“Architecture Review Board”的缩写）于2002年作为OpenGL的扩展发布。由于生产GPU芯片的企业数量很少，而且它们在硬件设计上都采用了相当相似的方法，因此这个共同开发的标准被所有企业采用。 

按照当时的标准，第一代ARB汇编语言还很粗糙：着色器在长度和复杂度上受到严格限制，不支持循环，条件语句的使用也非常有限。尽管如此，它让多重纹理处理变得更加简单和灵活，允许以新颖、非传统的方式使用纹理图像，甚至还能使用一些非常简单的过程纹理方法。GPU着色器编程很快流行起来，并通过创建扩展来增强着色器程序的功能。很快，着色器就要长得多，并且有分支和循环，而低级汇编语言对于应用程序程序员来说已经不够用了。

第一款用于GPU的高级着色器编程语言是Cg（“面向图形的C语言”），这是一种由英伟达设计的、具有类似C语言语法的编译型语言。由于Cg程序会被编译成标准化的ARB汇编指令，所以Cg也可用于对其他厂商的硬件进行编程。微软创建了自己的HLSL（“高级着色语言”），其语法非常相似，功能也相同。2004年，OpenGL发布了它的GLSL（“OpenGL着色语言”）。HLSL和GLSL都大量借鉴了Cg，而Cg又是基于Renderman着色语言（RSL）开发的，这么做是有充分理由的，因为事实证明RSL是一种非常有用的工具。


## 新的曙光

现代计算机架构在内存带宽方面面临着巨大的问题，对于图形处理器（GPU）来说尤其如此。许多并行处理核心可以集成在同一芯片上协同工作，以实现极高的理论处理速度。然而，在实际应用中，只有一小部分特定类型的问题能够达到这些理论速度：即那些对内存需求相对较小，并且处理单元之间很少或无需进行通信的问题。许多传统的计算问题并不属于这一类，因此不太适合大规模并行执行。不过，传统上用于实时渲染的那类算法确实属于这一范畴，这就是为什么拥有数百甚至数千个处理单元的GPU在提升实时3D计算机图形的质量和速度方面如此成功的原因。 

虽然实时渲染算法非常适合并行处理，基本上允许每个像素独立于其他所有像素进行计算，但大量纹理图像的处理却带来了一个问题。纹理数据需要多个处理单元同时访问，每个单元可能需要不同的数据块，而内存访问已成为瓶颈。有一些解决方法，包括采用本地缓存策略，甚至采用定制的内存架构，允许同时对不同地址进行多次读取，但内存访问仍然是一个根本性的阻塞点。

出于这个原因，大规模并行处理通常会涉及大量的空闲时间，在此期间执行单元等待数据，在数据到达之前无事可做。如果这些空闲时间能被无需访问任何内存的有用工作填满，大量未被利用的算力几乎可以免费使用。（这并非完全免费，因为忙碌的单元比空闲的单元消耗更多的电力，但不会花费更多时间。）这将大大降低大规模并行处理接近理论最大性能的难度。 

**通过程序化方法！**将纹理图像与计算出的程序化纹理混合，有可能在不增加处理时间的情况下提高渲染图像的复杂性和质量。在通常情况下，内存访问是渲染速度的主要限制因素，用程序化纹理替换一些纹理图像甚至可以提高速度，即使计算程序化纹理需要多花一些时间。使用预先计算好的数值查找表比每次执行时重新计算每个数值并在使用后丢弃的效率更低，这听起来有悖直觉，但在大规模并行执行环境中，这种情况可能发生，而且确实会发生。

除了在并行访问不同数据块时存在问题外，RAM存储体的内存访问速度相当慢（相对而言——从人类的角度来看，它仍然快得令人难以置信）。如果不以顺序方式访问数据，它还会有较高的延迟。由于现在对内部GPU寄存器进行操作的时钟速度已达到GHz级别，一次对RAM的访问可能需要几十个时钟周期，而在这段时间内实际上可以进行大量计算。在现代GPU中，只要有可能，利用简单的过程性模式绝对是个好主意。这里的“简单”是指“计算复杂度低”，即那些易于用算法形式描述且计算所需工作量合理的模式。正如我们将在后续章节中看到的，这类模式在视觉上不一定简单。 

所以，总结一下，如果你只需付出适度的工作量就能计算出某些内容，那么有时更好的办法是即时计算，使用后就丢弃，而不是将其存储在内存中。如果某个算法受内存延迟的影响，现代编译器可以通过重新排列底层指令的顺序来耍些“花招”，以便隐藏访问内存的时间，让执行过程在等待数据时不会停滞，但这并非总是可行。硬件辅助的计算机图形渲染往往严重受限于内存带宽。在这些情况下，未被利用的处理能力可以通过过程式方法来加以利用。 

## 技术现状
在撰写本文时（2025年），即使在低端个人计算设备上，包括低成本笔记本电脑和廉价智能手机，着色器可编程图形硬件也是标配。这种情况已经持续了相当长一段时间，足以淘汰那些不具备该功能的旧设备。高端和低端GPU的性能差异很大，但它们都可以使用相同的着色语言进行编程。OpenGL的JavaScript版本WebGL使用GLSL着色器。仍在一些应用程序中使用的HLSL与GLSL非常相似。基于Web的3D图形的一个仍在兴起的标准WebGPU有一种名为WGSL的着色语言，它也与GLSL非常相似。3D图形API Vulkan有望在长期内取代OpenGL，目前它使用GLSL作为其着色器编程的语言。尽管目前围绕GPU渲染有诸多令人兴奋之处，但软件程序化着色器依然不可或缺。开放标准“开放着色语言”（简称OSL）如今已完全取代了皮克斯的授权产品RSL，甚至在Renderman中也是如此。然而，它们之间的相似之处非常显著，无论是旧的RSL还是现代的OSL，在语法和着色器编写者可用的函数集方面都与GLSL颇为相似。OSL和GLSL均基于RSL。 

### 速度
在硬件渲染速度和处理能力方面，为游戏打造的高端个人电脑与低成本笔记本电脑或智能手机之间存在巨大差异。如今，几乎每台具备图形处理能力的设备都有某种专用图形处理器（GPU），但最出色的GPU拥有数千个处理单元，运行速度约为2GHz，而笔记本电脑的经济型GPU核心数量可能不到一百个，性能较弱、运行速度较慢且内存带宽较小。智能手机的GPU通常只有几个核心，因为它们必须体积小巧且功耗低。 
进化仍在继续，各类GPU的性能也在随着时间不断提升，但在任何时候，市场上两款产品的原始速度都可能相差百倍甚至千倍，应用程序员需要考虑到这一点。
具体来说，一块当前的游戏图形处理器（GPU），其允许的散热功率可达500瓦，并且通常是计算机中最昂贵的组件，每秒能够处理数百亿像素。这应该与以4K分辨率和每秒60帧的速度直接渲染动画交互内容的需求进行对比，后者 “仅” 相当于每秒输出5亿像素。高端GPU存在相当大的额外处理能力，可用于通过各种方式提高图像质量。 

### 真实感

软件渲染在很久以前就实现了照片级真实感。除了像合成人类演员这类观众对细节极为挑剔的应用场景外，在制作精良的视觉特效中，人们已无法分辨真假。硬件渲染目前还没那么出色，但正在迅速赶上。 
实时图形与预渲染内容之间的视觉质量差距正在缩小，而且并不总是能够区分两者。一些现代计算机游戏画面比许多普通的预渲染内容更好，而且一些预渲染内容出于预算原因或为了营造 “复古” 风格，会特意采用简约的手法。如今，游戏引擎正被用于借助GPU渲染电影级内容，虽然速度有所降低，但质量更高。在技术发展方面，我们已经达到了这样一个阶段：3D图形的质量不再像以往那样受到技术限制或输出格式的严重制约。现在，预算、创造力和技术是影响质量的主要限制因素，而且不久之后，它们将成为唯一的限制因素。

### 趋势

自从引入GPU以来，它们也一直被用于通用计算。起初，这需要一些变通方法和技巧，将输入和输出表示为图形数据，因为这是现有编程接口唯一能处理的数据类型。然而，GPU制造商逐渐接受了其硬件的双重角色，如今GPU常被用于纯计算，其输入和输出都与图形毫无关系。专门为GPU辅助计算开发了新的编程语言，并且GPU也在不断发展，以支持比图形计算所需更高的数值精度。 

某些非图形算法很适合大规模并行实现，比如最近 “人工智能” 领域 “深度学习” 算法的发展趋势，大量的 GPU 硬件集群被用于处理这些算法训练所需的海量数据。（说到人工智能，目前这类算法远称不上真正的人工智能。它只是运用统计学以及一层语义来处理文字、图像、声音、音乐或任何算法设计要处理的内容。本质上，这些算法仍然是派生而来的，并不智能，既无创造力也不聪明。但咱们言归正传。）

目前，在架构上使图形处理器（GPU）变得更加通用和灵活已成为一种明显的趋势，部分原因在于它们正被用于通用计算，同时也因为实时计算机图形学正在涉足以前专为离线渲染保留的领域。最显著的变化是，GPU 现在能够通过光线追踪来渲染场景，尽管与在 CPU 软件中执行的传统离线光线追踪相比，其方式仍受到更多限制。传统的硬件渲染只需从场景中获取少量局部数据就能渲染一个像素。然而，光线追踪是一种全局光照渲染方法，为处理单个像素，它需要访问大量关于场景的信息。此外，要精确预测每个处理单元具体需要哪些数据也极为困难，这使得为快速高效地访问数据做好准备变得更加艰难。基于这些原因，制造商不得不重新思考。 

一些以前只能用于离线渲染的计算机图形学方法，如今已可实际应用于实时渲染。光线追踪就是这样一种方法，近年来它受到了极大关注。另一个例子是程序纹理，尽管到目前为止它受到的关注很少。虽然作者肯定有所偏向，但我认为它值得在实时应用中得到更多探索和应用。

## 优缺点

程序纹理绘制是一种专门的工具，并非通用方法。显然，另一种选择是基于图像的纹理绘制，但每种方法都各有优缺点。这两种方法都值得考虑，也可以结合使用。

### 复用性
数字图像可以描绘任何二维图案，然而并非所有内容都能以算法形式描述，至少不容易做到。基于图像的纹理总能胜任这项工作，即便它们并非完全适合该任务。过程纹理则并非如此，这可能也是过程纹理没有得到更广泛应用的主要原因之一。你总是可以简单地将图像纹理应用到三维模型上，但你不能将过程纹理用于所有事物。 

### 灵活性

数字图像可以描绘任何二维图案，然而并非所有内容都能以算法形式描述，至少不容易做到。基于图像的纹理总能胜任这项工作，即便它们并非完全适合该任务。过程纹理则并非如此，这可能也是过程纹理没有得到更广泛应用的主要原因之一。你总是可以简单地将图像纹理应用到三维模型上，但你不能将过程纹理用于所有事物。 

### 品质

似乎如果纹理是来自现实世界的数码照片，照片级真实感会更容易实现，但数字图像有其固有的局限性。其中最突出的就是分辨率。一幅数字图像由采样像素组成，它无法表现比单个像素还小的细节。近距离观察带有图像纹理的表面，会导致图案变得块状或模糊。在离线制作中，可以避免或提前规划极端特写镜头，但实时交互式渲染，比如在电脑游戏中，并不总能预判用户想去哪里并进行近距离观察。（有些人，比如我自己，总会在游戏里花些时间走到不该去的地方看看，就为了好玩。）

过程式图案可以具有实际上“无限的分辨率”，因为它们是在渲染时根据所需的分辨率进行计算的。这与二维对象图形的工作方式类似。在对象图形中，你将对象的轮廓描述为理想曲线，然后将这些形状按照当前输出设备上当前视图所需的分辨率渲染成像素。这种技术在二维图形的多种应用中经常使用，可能最常见的是用于渲染文本，而且没有人对此有任何疑虑。它只是某些工作的最佳工具。 

分辨率是视觉质量最重要的方面之一。纹理图像可以具有非常高的分辨率，但其分辨率始终是有限的。程序生成的图案则不存在这个问题。当你放大时，边缘可以保持清晰，并且可以根据观看情况的需要动态添加更多细节。质量也可以根据输出设备的处理能力进行动态调整。


### 速度

在存储于随机存取存储器（RAM）中的纹理图像中查找像素值，这看似并非复杂操作，但它涉及内存访问，而随机存取存储器的速度落后于处理器速度。中央处理器（CPU）或图形处理器（GPU）中的一个时钟周期不到一纳秒，但随机存取存储器的一次读数可能需要几十纳秒甚至更长时间，具体取决于数据的访问方式。此外，在大规模并行执行环境中，当许多处理单元想要从同一内存获取不同数据时，内存读取可能会导致拥塞。与 “常识” 相悖的是，有时计算一个简单的程序生成图案比进行一次内存查找速度更快。鉴于大多数基于图像的纹理涉及多个图像以及它们之间的混合操作，基于图像的纹理处理可能需要大量时间。程序生成图案的执行速度可能更快。 现代GPU经过优化，能够高效处理基于图像的纹理，因此在大多数情况下，程序生成的图案并不会显著提升速度。不过，它们也并非总会拖累性能，所以值得考虑使用。 

### 效率
一款现代游戏，或者涉及3D效果的现代电影制作，需要大量的“资源”，其中大部分通常由纹理图像组成。程序纹理是一小段代码，与低分辨率数字图像相比，它是一种极为紧凑的图案表示形式。在需要考虑原始数据量的应用场景中，程序化方法确实能发挥出色。现代高品质游戏很少通过物理介质发行，而是需要下载多达几十GB的数据，这很不方便。如果需要通过无线网络以较低速度传输内容，甚至可能给接收方带来可观的成本，这个问题会更加严重。 

考虑到图形硬件的效率，需要注意的是，现代GPU中有很大一部分芯片面积用于处理纹理图像。如果将这部分芯片面积哪怕一小部分用于专门的功能来加速程序纹理的处理，比如加入一个哈希函数（见第8章“随机性”），甚至可能再加入一些专门的“噪声单元”（第10章“噪声”），那么程序纹理处理将会更具竞争力。 


### 可行性
在推广程序纹理作为一种方法时（这实际上就是我们在这里所做的事情），一个常见的反对意见是它很难实现。可以说，无论采用何种方式，三维图形的制作都并非易事，但这一观点确实有其道理。程序纹理是程序，与数字图像相比，创建它们需要不同类型的才能。能够将编程和数学作为视觉艺术创作工具的人，远没有能够使用二维图像编辑软件进行绘画和编辑图像的人那么容易找到，而且即使是经验丰富的着色器编写者，创建一个程序图案也可能需要花费大量的时间和精力。此外，参数化程序着色器并不总是能轻松适配现有的制作环境。改变往往很困难，而旧习惯也很难根除。 



如上文所述，既具备数学技能，又有视觉创意的欲望和天赋的程序员并非轻易就能找到。然而，经验表明，这样的人确实存在（本书的读者就是很好的例子），而且在某些情况下，他们能创造奇迹。着色器编程可能颇具难度，但对于有这方面天赋的人来说，这是一项有趣且有回报的工作。此外，纹理图像通常是一次性产品，只为特定场合创建过程着色器——和所有程序代码一样——经过一些细微修改后，或许可以在另一种完全不同的场景中重复使用。

在性能仍然是个问题且程序纹理不可行的情况下，可以根据需要在客户端将它们渲染为普通纹理图像。由于硬件着色遵循普遍采用的标准，那些没有足够处理能力实时渲染程序纹理的平台，仍然有能力提前离线渲染它们。因此，当输出设备允许时，程序方法可用于实现最高质量，但对于无法全速处理它们的低性能设备，也能很好地进行适配。 


## 免责声明

前两节“技术现状”和“利弊”包含了一些随时间推移会发生变化的观察结果和数据，以及对未来技术发展的一些假设。与所有对未来的预测一样，这些内容可能会被证明是错误的。从现在起多年后，一般性的讨论或许仍然有效，但数据和结论可能会发生变化。不过，如果让我根据现有信息猜测的话，进一步的发展并不会降低过程式方法的实用性。就我个人而言，我认为随着时间推移，它们可能会变得更有用。 

当然，这只是我的观点，但我写这本书是因为我认为我是对的。
